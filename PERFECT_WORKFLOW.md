# Perfect Workflow for Automated Boone Rankings

Since you're happy to provide the URLs once per week, here's the **perfect solution** that combines the best of both worlds:

## ğŸ¯ **Your Ideal Workflow**

### **Weekly Setup (5 minutes once per week):**
```bash
# 1. Set this week's Yahoo article URL and your Datawrapper URLs
python3 simple_automated_scraper.py --setup
```

### **Automatic Operation:**
- âœ… System monitors the Yahoo article timestamp every hour
- âœ… When Boone updates â†’ automatically scrapes your configured URLs
- âœ… Updates rankings.json automatically  
- âœ… No further manual work needed

## ğŸ”§ **Easy Weekly Setup**

Instead of the interactive setup, just edit the config file directly:

**File: `simple_automation_config.json`**
```json
{
  "monitor_url": "https://sports.yahoo.com/fantasy/article/week-1-rankings-...",
  "week_number": 1,
  "datawrapper_urls": {
    "qb": ["https://datawrapper.dwcdn.net/bqgx9/"],
    "rb": ["https://datawrapper.dwcdn.net/ABC123/"],
    "wr": ["https://datawrapper.dwcdn.net/XYZ789/"],
    "te": ["https://datawrapper.dwcdn.net/DEF456/"],
    "flex": ["https://datawrapper.dwcdn.net/GHI789/"],
    "def": ["https://datawrapper.dwcdn.net/JKL012/"],
    "k": ["https://datawrapper.dwcdn.net/MNO345/"]
  }
}
```

## ğŸ“‹ **Weekly Checklist**

**Every Week (5 minutes):**
1. âœ… Find Boone's new rankings article URL
2. âœ… Right-click each ranking table â†’ Inspect â†’ Copy Datawrapper iframe URLs  
3. âœ… Update `simple_automation_config.json` with new URLs
4. âœ… Done! System automatically handles the rest

**System Automatically:**
1. âœ… Monitors Yahoo article timestamp every hour  
2. âœ… When timestamp changes â†’ scrapes all your configured URLs
3. âœ… Updates rankings.json with fresh data
4. âœ… Creates backups automatically

## ğŸš€ **Commands You'll Use**

```bash
# Test the system works
python3 simple_automated_scraper.py --dry-run

# Force a test scrape  
python3 simple_automated_scraper.py --force

# Normal automated run (only scrapes if timestamp changed)
python3 simple_automated_scraper.py

# Set up hourly automation
crontab -e
# Add: 0 6-18 * * * cd /path/to/boone_rankings_app && python3 simple_automated_scraper.py >> simple_automation.log 2>&1
```

## ğŸ‰ **Why This Is Perfect For You**

âœ… **5 minutes of setup per week** - Just update URLs  
âœ… **100% automated monitoring** - No email notifications needed  
âœ… **Uses your working scraper logic** - Same code that works for QB  
âœ… **Reliable timestamp detection** - Proven to work  
âœ… **Complete automation** - Rankings update within an hour of Boone's changes  
âœ… **Same JSON format** - Works with your existing site  
âœ… **Backup system** - Never lose data  
âœ… **Detailed logging** - Easy to troubleshoot  

## ğŸ“ **File Structure**

```
boone_rankings_app/
â”œâ”€â”€ simple_automated_scraper.py     # Main automation script
â”œâ”€â”€ simple_automation_config.json   # Weekly URL configuration  
â”œâ”€â”€ rankings.json                   # Auto-updated rankings
â”œâ”€â”€ simple_automation.log          # Automation logs
â””â”€â”€ rankings.json.backup.*         # Automatic backups
```

## ğŸ”„ **Migration from Current System**

1. **Keep your current system running**
2. **Test this alongside** with different config file
3. **Once confident, switch over**
4. **Disable old Netlify functions** (optional)

This gives you the **perfect balance**:
- **Minimal weekly effort** (just URL updates)  
- **Maximum automation** (everything else automatic)
- **100% reliability** (uses proven scraping code)

**Ready to set this up?** Just update the config file with your URLs and test it! ğŸ¯